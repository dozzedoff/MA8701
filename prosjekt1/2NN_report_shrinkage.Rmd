---
title: "Project 1 - Shrinkage"
author: "2NN: Bjerke, Martin & Spremic, Mina"
date: "2/15/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Matrix)
library(glmnet)
library(grplasso)
library(gglasso)
set.seed(3012)
```



# Introduction

The goal of this project is to apply the shrinkage methods to a dataset of the groups choice. One of the main methods of the first part of the course has been lasso, which is often applied in cases when the number of covariates is much larger than the number of observations. Therefore, we have chosen a dataset satisfying these criteria. 

# Dataset
The dataset in question is a Gastrointestinal Lesions in Regular Colonoscopy dataset, which can be accessed at https://archive.ics.uci.edu/ml/datasets/Gastrointestinal+Lesions+in+Regular+Colonoscopy.

This dataset contains a response variable, which is one of the three types of lesions (growth): hyperplasic, adenoma and serates, with the first one being benign and the latter two malignant. 
The rest of the covariates, 699 of them, have been extracted from video, more specificly from a colonoscopy (a camera being sent through the bowels), and represent different aspects of it, through rotation, texture, colour, contrast etc. They are divided roughly in three groups, which are textural features, color features and shape features, all having multiple subgroups. 

This dataset had to be labelled all over again, as the .txt file including the data did not have a header. Relabelling code is available in the [git-repo](https://github.com/dozzedoff/MA8701/tree/main/prosjekt1). 
In this dataset, majority of the 699 covariates have not been easy to decipher, both due to their large number and the lack of information on them, both on the webpage of the dataset, but also in the paper that the dataset is connected to. Most of the information about the variables have been references to other papers which do analysis and describe the process of extracting some of these variables from video. We will include references to some of these articles in the end of the report. The dataset is connected to a published paper [Computer-Aided Classification of Gastrointestinal Lesions in Regular Colonoscopy](https://hal.archives-ouvertes.fr/hal-01291797v2/document). 

The names of the covariates, which have been used as labels are the following: 

Group | Subgroup | Number of covariates
--- | --- | ---
Type of light used for recording\*|| 1
2D textural features||422
|| AHT - Autocorrelation Homogeneous Texture
|| (Invariant Gabor Texture)  | 166
||Rotational Invariant LBP| 256
2D color features||76
||Color naming|16
||Discriminative Color|13
||Hue|7
||Opponent|7
||Color gray-level 
||co-occurence matrix|33
3D shape features || 200
|| shapeDNA | 100
|| KPCA | 100

\*Types of the light are 1=WL (White Light), 2=NBI(Narrow Band Imaging)



We wish to specify that this is a classification problem. Provided in the dataset are three clases of the response. We have chosen not to use all of the three classes in our problem, and have instead merged two of the classes. Instead of there being classes: hyperplasic, adenoma and serates, we have malignant and benign instead.  
An observations made is that some of the covariates had only zero entries, but due to lack of documentation on the cause of this, we have chosen to keep all the covariates in the dataset. Finally, it is worth mentioning that there actually are only 76 unique cases in the dataset, that have been measures twice, using different light.



# Analysis

The problem at hand is a classification problem. As such, we have chosen to apply logistic regression, lasso and group lasso. We attampt using logistic regression, hoping it can serve as sort of reference. The reason behind choosing lasso is due to the large number of covariates in the dataset, which we hope could be shrunk. Group lasso is chosen due to a natural grouping of the variables due to the structuring of the dataset.  

We show a contigency table, showing the proportion of the number of positive and negative responses from the dataset and present some simple summary of variables representing each of the main groups. Due to the large number of covariates, and inability to deem some of them more important than others, due to the lack of domain knowledge, we conclude the exploratory analysis with that. If the reader is interested in all of the covariates, we suggest they inspect the rest of the covaraites on their own initiative.


```{r data, include=FALSE}
data_set <- read.table("gastroenterology_dataset/data.txt", header= FALSE, sep=",", dec =".", colClasses = "numeric")
df_gl <- as.data.frame(data_set)
df_gl <- as.data.frame(t(df_gl))
names(df_gl)[1] <- "type_of_lesion"
names(df_gl)[2] <- "type_of_light_1_WL_2_NBL"
names(df_gl)[3:168] <- paste0("Textural_feature_AHT",1:166)
names(df_gl)[169:424] <- paste0("Rot_invariant_LBP", 1:256)
names(df_gl)[425:440] <- paste0("Color_naming", 1:16)
names(df_gl)[441:453] <- paste0("Discriminative_color", 1:13)
names(df_gl)[454:460] <- paste0("Hue", 1:7)
names(df_gl)[461:467] <- paste0("Opponent", 1:7)
names(df_gl)[468:500] <- paste0("Col_gray_lvl_co-occurr_mx", 1:33)
names(df_gl)[501:600] <- paste0("shapeDNA", 1:100)
names(df_gl)[601:700] <- paste0("KPCA", 1:100)

test_ds <- df_gl

test_ds$type_of_lesion[which(test_ds$type_of_lesion==1)]<-0
test_ds$type_of_lesion[which(test_ds$type_of_lesion==3)]<-1
test_ds$type_of_lesion[which(test_ds$type_of_lesion==2)]<-1

```


```{r mal_ben, echo=TRUE}

table(test_ds$type_of_lesion, dnn=c("Benign Malignant"))

summary(test_ds[,c(1,2,3,169,425,441,454,461,468,501,601)])

```

## Logistic regression

Unfortunately, the logistic regression did not manage to run, and we have recieved an error: *does not converge*, indicating that we most probably have multicollinearity issues and the issue with number of covariates being too large comapared to the number of observations. We have attempted to surpass this by increasing the number of iterations, but even though it then converges, it does not provide meaningful results, with very many coefficients being set to NA.

```{r include=FALSE}
# design matrix
x_no_zero <- model.matrix(type_of_lesion~.,data=test_ds)[,-1]
x_no_zero_scale_1 <- scale(x_no_zero)
x_no_zero_scale_1[is.nan(x_no_zero_scale_1)] <- 0
#x_no_zero_new <- x_no_zero[,-(which(colSums(x_no_zero)==0))]
#stdz_xs <- scale(x_no_zero_new)

# response
ys <- test_ds[,1]
index_s <- c(1,rep(2,166),rep(3,256),rep(3,16),rep(4,13), rep(5,7), rep(6,7), rep(7,33),rep(8,100),rep(9,100))

##### group lasso from grplasso #####
#lmax <- lambdamax(x=x_no_zero_scale, y=ys,index=index_s,penscale=sqrt, model = LogReg(), center = FALSE,
#                  standardize = FALSE)*0.5^(0:699)
#grplasso_test <- grplasso(x_no_zero_scale, ys, model = LogReg(), penscale=sqrt, center = FALSE, standardize = FALSE, lambda=lmax, index=index_s)

# needs the reponse to be on the form -1 and 1
ys_gl <- ys
ys_gl[which(ys_gl==0)]<- -1

```

## Lasso

We proceed applying the lasso, as implemented in the glmnet package.

Cross-validation is performed using the cv.lasso function from glmnet package in order to find the optimal lambda.
We plot the lasso-plot, visualising at which $\lambda$s the coefficients get shrunk to zero, and how many of them have not been shrunken.

Additionally we show the value minimum $\lambda$ as well as one standard deviation $\lambda$.

```{r lasso, echo=FALSE}
set.seed(3012)
lassofit=glmnet(x=x_no_zero_scale_1,y=ys,alpha=1,standardize=FALSE,family="binomial") # already standardized
#plot(lassofit,xvar="lambda",label=TRUE)
#mtext("Lasso",font=2, side = 3, line = -2, outer = TRUE)

# question is whether 
cv.lasso=cv.glmnet(x=x_no_zero_scale_1,y=ys,alpha=1,standardize=FALSE,family="binomial",type.measure = "class")
print(paste("The lamda giving the smallest CV error: ",cv.lasso$lambda.min))
print(paste("The one standard deviation lambda: ",cv.lasso$lambda.1se))



#abline(v=log(cv.lasso$lambda.min))

```


```{r echo=FALSE, fig.cap="The plot shows the number of coefficients that have not been shrunken, for the optimal lambda - one standard deviation."}
plot(lassofit,xvar="lambda",label=TRUE)
mtext("Lasso",font=2, side = 3, line = -2, outer = TRUE)
abline(v=log(cv.lasso$lambda.1se))
```

```{r echo=FALSE, fig.cap="The plot shows the log-lambda and the corresponding misclassification error. Additionally on the top we see how many covariates minimum lambda and one standard deviation lambda give."}
plot(cv.lasso)
```



We can see the covariates picked with the lasso method. These are: 6 different textural feature AHT number 9, 98, 110, 120, 132, 135, rotationally invariante LBP number 4 and 52. It shows discriminative_color 10 is non-zero. In addition three of the covariates realted to the level of grayscale are included as well, namely 7, 25 and 32. Finally shapeDNA number 72 and KPCA number 2 are included.  It is rather difficult to say whether these results were as expected, considering the lack of knowledge about the covariates. We could perhaps speculate that, for example, grey level scale indicates some sort of contrast in the video/pictures, giving certain indication of edges and features which are more pronounced for malignant lesions.
```{r echo=FALSE}
# Lasso method results of coefficients picked, nice print-out

lasso_coef <- coef(lassofit,s=cv.lasso$lambda.1se)
lasso_coef_print <- cbind(names(test_ds[lasso_coef@i+1]),lasso_coef@x)
lasso_coef_print[1,1] <- "Intercept"
lasso_coef_print


```
## Group Lasso

We proceed to apply the group lasso to our dataset. The groups chosen are structured such that the covariates belonging to the group of variables presented in the beginning, are in the same group in this analysis as well. This has been the only sensible decision of a group split, taken into considerationg the number of covariates and the difficulties concerning decyphering their concrete meaning. 


```{r , echo=FALSE}
set.seed(3012)
cv_testing_gglasso <- cv.gglasso(x=x_no_zero_scale_1, y=ys_gl, group=index_s, nfolds=5, loss="logit")
```

```{r grouplasso, echo=FALSE, fig.cap="The plot shows different groups of coefficients, and lambdas at which they are being shrunken. The vertical line indicates the one standard deviation lambda obtained through cross validation. "}
plot(cv_testing_gglasso$gglasso.fit)
abline(v=log(cv_testing_gglasso$lambda.1se))
```

```{r cvgrouplasso, echo=FALSE, fig.cap="The plot shows the log-lambda and the corresponding misclassification error. Additionally on the top we see how many groups of covariates minimum lambda and one standard deviation lambda give."}
plot(cv_testing_gglasso)
```

```{r ,echo=FALSE}
group_lasso_coef <- coef(cv_testing_gglasso$gglasso.fit, s=cv_testing_gglasso$lambda.1se)
coef_value_gglasso <- group_lasso_coef[which(group_lasso_coef!=0)]
coef_name_gglasso <- row.names(group_lasso_coef)[which(group_lasso_coef!=0)]
# KPCA, color gray level co-occurence, textural feature, type of light
print_coef_gglasso <- rbind(coef_name_gglasso[2],coef_name_gglasso[3], coef_name_gglasso[169],coef_name_gglasso[202])
print_coef_gglasso

```

The covariate groups chosen by the group lasso are as given above. If we compare it to the ones chosen by the lasso, we observe that the groups of covariates chosen by the group lasso are represented among the covariates which have not bene shrunken by the lasso. The difference between the two is that the group lasso picks the covariate indicating type of light but not the rotational invariance, discriminateive color and shapeDNA.

# Prediction

We wish to point out that due to a low number of observations, we do not believe splitting the dataset into training and test set is feasible. We believe we would not be able to obtain a realistic representation of the fraction of malignant lesions in population with so few observations in both train and test set. Hence we will not be including any prediction, but will do some inference in the next section.


# Inference

For the inference section we choose to do bootstrapping on the lasso, in order to infere whether the covariates that have not been shrunken, have an indication of actually being significant.

```{r bootstrap, echo=FALSE}
set.seed(3012)
B=1000
n=nrow(x_no_zero_scale_1)
p=ncol(x_no_zero_scale_1)
lasso_mx=matrix(ncol=p+1,nrow=B)
# no need or separate function for steps 1-6 since can use cv.glmnet
# and weight argument for giving the new bootstrapped data
for (b in 1:B)
{
  #print(b)
  idx=sort(sample(1:n,replace=TRUE))
  wids=rep(0,n)
  for (i in 1:n)
    wids[i]=sum(idx==i)
  boot=cv.glmnet(x_no_zero_scale_1,ys,weights=wids,standardize=FALSE,family="binomial",type.measure = "class")
  lasso_mx[b,]=as.vector(coef(boot)) #automatic lambda 1sd
}

investigate_mx <- lasso_mx[,which(lasso_coef != 0)]
colnames(investigate_mx)=c(lasso_coef_print[,1])


```



```{r echo=FALSE, fig.cap="Boxplot for coefficients for 14 covaraites including the intercept."}
boxplot(investigate_mx)  
#boxplot(investigate_mx[,8:15])


```

```{r echo=FALSE, fig.cap="Boxplot showing the latter 8 coefficients, making it easier to see the coefficient which do not have median at zero. Due to the long covariate names, we provide these here, in order of appearance in the boxplot: Rot_invariant_LBP4, Rot_invariant_LBP52, Discriminative_color10, Col_gray_lvl_co-occurr_mx7, Col_gray_lvl_co-occurr_mx25, Col_gray_lvl_co-occurr_mx32, shapeDNA72, KPCA2."}

boxplot(investigate_mx[,8:15])


```

```{r echo=FALSE, fig.cap="Barplot showing the proportion of time each of the covariates is zero."}
lasso0perc=apply(abs(investigate_mx)<.Machine$double.eps,2,mean)
barplot(lasso0perc)
```

From the boxplots we observe that majority of the boxes have median at zero, or are very close to zero. This is further confirmed throught the barplot. In the barplot we can see that almost all of the coefficients are zero at least 40\% of the time. The only coefficient that is noticeably different, and is almost never zero, is one of the coefficients for the gray-level contrast, followed by one of the coefficients for rotational invariance and another gray-level contrast coefficient being zero between 20 and 40\% of the time.

# Discussion and concluding words

After the analysis has been conducted, and inference has been performed, the results were slightly suprising. Even though we did not know exactly what to expect due to the large number of covariates, most of which have been extracted from videos, as mentioned previously, it was rather suprising only one of the coefficients was almost never zero, while in total only 3 were non-zero less than 40\% of the time. 
We could speculate whether the results would have been different have we had opted out for a classification problem with three classes. 


# References
[1]: "Gastrointestinal Lesions in Regular Colonoscopy Data Set"  - https://archive.ics.uci.edu/ml/datasets/Gastrointestinal+Lesions+in+Regular+Colonoscopy  
[2]: "Computer-Aided Classification of Gastrointestinal Lesions in Regular Colonoscopy" https://pubmed.ncbi.nlm.nih.gov/28005009/  
[3]: R. Nava, G. Cristo bbal, and B. Escalante-Ramiırez, “Invariant texture analysis through local binary patterns,” CoRR, vol. abs/1111.7271, 2011.  
[4]: F. Riaz, F. B. Silva, M. Dinis-Ribeiro, and M. T. Coimbra, “Invariant gabor texture descriptors for classification of gastroenterology images,” IEEE Trans. Biomed. Engineering, vol. 59, no. 10, pp. 2893–2904, 2012.  
[5]: M. Reuter, F.-E. Wolter, and N. Peinecke, “Laplace–beltrami spectra as shape-dnaof surfaces and solids,” Computer-Aided Design, vol. 38,
no. 4, pp. 342–366, 2006.  
[6]:C. M. Bishop, Pattern recognition and machine learning. Springer,
2006.  
[7]: grplasso package https://cran.r-project.org/web/packages/grplasso/grplasso.pdf  
[8]: gglasso package https://cran.r-project.org/web/packages/gglasso/gglasso.pdf  
[9]: glmnet package https://cran.r-project.org/web/packages/glmnet/glmnet.pdf  
[10]: matrix package https://cran.r-project.org/web/packages/Matrix/Matrix.pdf  

